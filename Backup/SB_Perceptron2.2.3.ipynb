{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "religious-absence",
   "metadata": {},
   "source": [
    "# SSB_5350_Perceptron2.2 \n",
    "### Scott Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wound-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reflected-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-expert",
   "metadata": {},
   "source": [
    "# Define the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "downtown-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weights(w, act):\n",
    "    err = 0\n",
    "    for i in range(len(piv)):\n",
    "       vs = weighted_sum(piv[i], w[1:]) + weights[0]\n",
    "       if act(vs, threshold) != labels[i]:\n",
    "           err+=1\n",
    "    print(err/len(piv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "three-entry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(cycles):\n",
    "    step =  2.0\n",
    "    for c in range(cycles):\n",
    "        #initialize error to 0 for each cycle\n",
    "        num_errors = 0\n",
    "        \n",
    "        for  i in range(len(piv)):\n",
    "            w_sum = weighted_sum(piv[i], weights[1:])\n",
    "            #add bias whdi is first weight\n",
    "            w_sum = weights[0] + w_sum\n",
    "            act = activation(w_sum, threshold)\n",
    "            #determine if the weights yeild right answers\n",
    "            # if so then erro 0 or no error\n",
    "            error = labels[i] - act\n",
    "            \n",
    "            if error != 0:\n",
    "                #increase error count\n",
    "                num_errors += 1\n",
    "#update weights to minimize error in  next cycle\n",
    "                #update weights to minimize error in next cycle\n",
    "                weights[0] += step * error * piv[i][0]\n",
    "                weights[1] += step * error * piv[i][1]\n",
    "#update bias\n",
    "                weights[0] += error\n",
    "        #end fit early if a cycle had no errors\n",
    "        print(c, num_errors)\n",
    "                \n",
    "        if num_errors == 0:\n",
    "            print(c, weights, bias)\n",
    "            break\n",
    "    else:\n",
    "        print(\"no answer found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increasing-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(vec, weights):\n",
    "    w_vec = vec*weights;\n",
    "    return w_vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "differential-essay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(val, threshold):\n",
    "    if val >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specific-jerusalem",
   "metadata": {},
   "source": [
    "# The Perceptron Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "internal-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "precise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "labels = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "robust-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some random precise data\n",
    "precise_labels = []\n",
    "precise_rounds = []\n",
    "for i in range(20000):\n",
    "    large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "    donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "    small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "    reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "    precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "    precise_labels += [1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "human-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data                               \n",
    "ratio = int(len(precise_rounds)*.7)           \n",
    "train_piv = np.array(precise_rounds[:ratio])   \n",
    "train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "test_piv = np.array(precise_rounds[ratio:])   \n",
    "test_labels = np.array(precise_labels[ratio:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "destroyed-forward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 57\n",
      "1 5\n",
      "2 5\n",
      "3 0\n",
      "3 [-26.05345003  18.07272694   0.91970309] 0.19528784230202723\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "piv = train_piv\n",
    "labels = train_labels\n",
    "bias = random.uniform(-1.0,1.0)\n",
    "#add bias on the end. Some people add bias on the beginning.\n",
    "weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "threshold = 2.0\n",
    "lifecycles = 20\n",
    "fit(lifecycles)\n",
    "#must activate weighted test values to turn the back to lables 0 or 1\n",
    "piv = test_piv\n",
    "labels = test_labels\n",
    "test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "medieval-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percept():\n",
    "    # Define the input\n",
    "    precise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "    labels = np.array([0, 0, 0, 1])\n",
    "    \n",
    "    #make some random precise data\n",
    "    precise_labels = []\n",
    "    precise_rounds = []\n",
    "    for i in range(20000):\n",
    "        large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "        donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "        small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "        reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "        precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "        precise_labels += [1,1,0,0]\n",
    "    #split the data                               \n",
    "    ratio = int(len(precise_rounds)*.7)           \n",
    "    train_piv = np.array(precise_rounds[:ratio])   \n",
    "    train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "    test_piv = np.array(precise_rounds[ratio:])   \n",
    "    test_labels = np.array(precise_labels[ratio:])\n",
    "    \n",
    "    piv = train_piv\n",
    "    labels = train_labels\n",
    "    bias = random.uniform(-1.0,1.0)\n",
    "    #add bias on the end. Some people add bias on the beginning.\n",
    "    weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "    threshold = 2.0\n",
    "    lifecycles = 20\n",
    "    fit(lifecycles)\n",
    "    #must activate weighted test values to turn the back to lables 0 or 1\n",
    "    piv = test_piv\n",
    "    labels = test_labels\n",
    "    test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "deluxe-patrick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 [-26.05345003  18.07272694   0.91970309] 0.19528784230202723\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "percept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "standard-cathedral",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percept1(pi):\n",
    "    # Define the input\n",
    "    precise_input_vals = pi\n",
    "    labels = np.array([0, 0, 0, 1])\n",
    "    \n",
    "    #make some random precise data\n",
    "    precise_labels = []\n",
    "    precise_rounds = []\n",
    "    for i in range(20000):\n",
    "        large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "        donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "        small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "        reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "        precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "        precise_labels += [1,1,0,0]\n",
    "    #split the data                               \n",
    "    ratio = int(len(precise_rounds)*.7)           \n",
    "    train_piv = np.array(precise_rounds[:ratio])   \n",
    "    train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "    test_piv = np.array(precise_rounds[ratio:])   \n",
    "    test_labels = np.array(precise_labels[ratio:])\n",
    "    \n",
    "    piv = train_piv\n",
    "    labels = train_labels\n",
    "    bias = random.uniform(-1.0,1.0)\n",
    "    #add bias on the end. Some people add bias on the beginning.\n",
    "    weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "    threshold = 2.0\n",
    "    lifecycles = 20\n",
    "    fit(lifecycles)\n",
    "    #must activate weighted test values to turn the back to lables 0 or 1\n",
    "    piv = test_piv\n",
    "    labels = test_labels\n",
    "    test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "selective-dialogue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 [-26.05345003  18.07272694   0.91970309] 0.19528784230202723\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "percept1(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "marked-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percept2(pi,lb):\n",
    "    # Define the input\n",
    "    precise_input_vals = pi\n",
    "    labels = lb\n",
    "    \n",
    "    #make some random precise data\n",
    "    precise_labels = []\n",
    "    precise_rounds = []\n",
    "    for i in range(20000):\n",
    "        large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "        donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "        small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "        reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "        precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "        precise_labels += [1,1,0,0]\n",
    "    #split the data                               \n",
    "    ratio = int(len(precise_rounds)*.7)           \n",
    "    train_piv = np.array(precise_rounds[:ratio])   \n",
    "    train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "    test_piv = np.array(precise_rounds[ratio:])   \n",
    "    test_labels = np.array(precise_labels[ratio:])\n",
    "    \n",
    "    piv = train_piv\n",
    "    labels = train_labels\n",
    "    bias = random.uniform(-1.0,1.0)\n",
    "    #add bias on the end. Some people add bias on the beginning.\n",
    "    weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "    threshold = 2.0\n",
    "    lifecycles = 20\n",
    "    fit(lifecycles)\n",
    "    #must activate weighted test values to turn the back to lables 0 or 1\n",
    "    piv = test_piv\n",
    "    labels = test_labels\n",
    "    test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "logical-wrong",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 [-26.05345003  18.07272694   0.91970309] 0.19528784230202723\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "l = np.array([0, 0, 0, 1])\n",
    "percept2(p,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "little-christianity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percept3(pi,lb):\n",
    "    # Define the input\n",
    "    precise_input_vals = pi\n",
    "    labels = lb\n",
    "    \n",
    "    #make some random precise data\n",
    "    precise_labels = []\n",
    "    precise_rounds = []\n",
    "    for i in range(20000):\n",
    "        large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "        donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "        small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "        reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "        precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "        precise_labels += [1,1,0,0]\n",
    "    #split the data                               \n",
    "    ratio = int(len(precise_rounds)*.7)           \n",
    "    train_piv = np.array(precise_rounds[:ratio])   \n",
    "    train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "    test_piv = np.array(precise_rounds[ratio:])   \n",
    "    test_labels = np.array(precise_labels[ratio:])\n",
    "    \n",
    "    piv = train_piv\n",
    "    labels = train_labels\n",
    "    bias = random.uniform(-1.0,1.0)\n",
    "    #add bias on the end. Some people add bias on the beginning.\n",
    "    #bias is the +1 after the lenght of the features\n",
    "    rand_weights = np.random.rand(len(precise_rounds[0]) + 1)\n",
    "    weights = (rand_weights*2)-1 #convert to a float between -1 and 1\n",
    "    #weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "    threshold = 1.0\n",
    "    lifecycles = 20\n",
    "    fit(lifecycles)\n",
    "    #must activate weighted test values to turn the back to lables 0 or 1\n",
    "    piv = test_piv\n",
    "    labels = test_labels\n",
    "    test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "seasonal-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 [-26.05345003  18.07272694   0.91970309] 0.19528784230202723\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "p = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "l = np.array([0, 0, 0, 1])\n",
    "percept3(p,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-rabbit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
