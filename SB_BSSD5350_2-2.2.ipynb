{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indonesian-volume",
   "metadata": {},
   "source": [
    "# SSB_5350_Perceptron2.2 \n",
    "### Scott Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "amended-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "progressive-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-linux",
   "metadata": {},
   "source": [
    "# Define the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nonprofit-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weights(w, act):\n",
    "    err = 0\n",
    "    for i in range(len(piv)):\n",
    "       vs = weighted_sum(piv[i], w[1:]) + weights[0]\n",
    "       if act(vs, threshold) != labels[i]:\n",
    "           err+=1\n",
    "    print(err/len(piv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "extra-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(cycles):\n",
    "    step =  2.0\n",
    "    for c in range(cycles):\n",
    "        #initialize error to 0 for each cycle\n",
    "        num_errors = 0\n",
    "        \n",
    "        for  i in range(len(piv)):\n",
    "            w_sum = weighted_sum(piv[i], weights[1:])\n",
    "            #add bias whdi is first weight\n",
    "            w_sum = weights[0] + w_sum\n",
    "            act = activation(w_sum, threshold)\n",
    "            #determine if the weights yeild right answers\n",
    "            # if so then erro 0 or no error\n",
    "            error = labels[i] - act\n",
    "            \n",
    "            if error != 0:\n",
    "                #increase error count\n",
    "                num_errors += 1\n",
    "#update weights to minimize error in  next cycle\n",
    "                #update weights to minimize error in next cycle\n",
    "                weights[0] += step * error * piv[i][0]\n",
    "                weights[1] += step * error * piv[i][1]\n",
    "#update bias\n",
    "                weights[0] += error\n",
    "        #end fit early if a cycle had no errors\n",
    "        print(c, num_errors)\n",
    "                \n",
    "        if num_errors == 0:\n",
    "            print(c, weights, bias)\n",
    "            break\n",
    "    else:\n",
    "        print(\"no answer found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "double-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(vec, weights):\n",
    "    w_vec = vec*weights;\n",
    "    return w_vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confirmed-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(val, threshold):\n",
    "    if val >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-immunology",
   "metadata": {},
   "source": [
    "# The Perceptron Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "awful-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "precise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "labels = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "private-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some random precise data\n",
    "precise_labels = []\n",
    "precise_rounds = []\n",
    "for i in range(20000):\n",
    "    large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "    donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "    small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "    reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "    precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "    precise_labels += [1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "engaging-tattoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data                               \n",
    "ratio = int(len(precise_rounds)*.7)           \n",
    "train_piv = np.array(precise_rounds[:ratio])   \n",
    "train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "test_piv = np.array(precise_rounds[ratio:])   \n",
    "test_labels = np.array(precise_labels[ratio:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ultimate-excess",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 505\n",
      "1 235\n",
      "2 154\n",
      "3 129\n",
      "4 117\n",
      "5 115\n",
      "6 108\n",
      "7 59\n",
      "8 77\n",
      "9 75\n",
      "10 73\n",
      "11 73\n",
      "12 75\n",
      "13 74\n",
      "14 74\n",
      "15 54\n",
      "16 68\n",
      "17 77\n",
      "18 59\n",
      "19 54\n",
      "no answer found\n",
      "0.000625\n"
     ]
    }
   ],
   "source": [
    "piv = train_piv\n",
    "labels = train_labels\n",
    "bias = random.uniform(-1.0,1.0)\n",
    "#add bias on the end. Some people add bias on the beginning.\n",
    "weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "threshold = 2.0\n",
    "lifecycles = 20\n",
    "fit(lifecycles)\n",
    "#must activate weighted test values to turn the back to lables 0 or 1\n",
    "piv = test_piv\n",
    "labels = test_labels\n",
    "test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ideal-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(piv,lbls):\n",
    "\t# Define the input\n",
    "\t#precise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]]\n",
    "\tprecise_input_labels = piv\n",
    "\t#labels = np.array([0, 0, 0, 1])\n",
    "\tlabels = lbls\n",
    "\tprint(\"piv: \", piv)\n",
    "\tprint(\"lbls: \", lbls)\n",
    "\n",
    "\t#make some random precise data\n",
    "\tprecise_labels = []\n",
    "\tprecise_rounds = []\n",
    "\tfor i in range(20000):\n",
    "\t\tlarge_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "\t\tdonut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "\t\tsmall_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "\t\treg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "\t\tprecise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "\t\tprecise_labels += [1,1,0,0]\n",
    "\n",
    "\t#split the data                               \n",
    "\tratio = int(len(precise_rounds)*.7)           \n",
    "\ttrain_piv = np.array(precise_rounds[:ratio])   \n",
    "\ttrain_labels = np.array(precise_labels[:ratio])\n",
    "\n",
    "\ttest_piv = np.array(precise_rounds[ratio:])   \n",
    "\ttest_labels = np.array(precise_labels[ratio:])\n",
    "\n",
    "\tpiv = train_piv\n",
    "\tlabels = train_labels\n",
    "\tbias = random.uniform(-1.0,1.0)\n",
    "\t#add bias on the end. Some people add bias on the beginning.\n",
    "\tweights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "\tthreshold = 2.0\n",
    "\tlifecycles = 20\n",
    "\tfit(lifecycles)\n",
    "\t#must activate weighted test values to turn the back to lables 0 or 1\n",
    "\tpiv = test_piv\n",
    "\tlabels = test_labels\n",
    "\ttest_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "presidential-backup",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piv:  [[1.   4.  ]\n",
      " [7.   0.1 ]\n",
      " [1.5  0.28]\n",
      " [1.1  0.8 ]]\n",
      "lbls:  [0 1 0 1]\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "no answer found\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "piv = np.array([[1,4], [7,.1], [1.5,.28], [1.1, .8]])\n",
    "lbls = np.array([0, 1, 0, 1])\n",
    "perceptron(piv,lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "southwest-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percept():\n",
    "\t# Define the input\n",
    "\tprecise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "\tlabels = np.array([0, 0, 0, 1])\n",
    "\n",
    "\t#make some random precise data\n",
    "\tprecise_labels = []\n",
    "\tprecise_rounds = []\n",
    "\tfor i in range(20000):\n",
    "\t\tlarge_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "\t\tdonut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "\t\tsmall_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "\t\treg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "\t\tprecise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "\t\tprecise_labels += [1,1,0,0]\n",
    "\n",
    "\t#split the data                               \n",
    "\tratio = int(len(precise_rounds)*.7)           \n",
    "\ttrain_piv = np.array(precise_rounds[:ratio])   \n",
    "\ttrain_labels = np.array(precise_labels[:ratio])\n",
    "\n",
    "\ttest_piv = np.array(precise_rounds[ratio:])   \n",
    "\ttest_labels = np.array(precise_labels[ratio:])\n",
    "\n",
    "\tpiv = train_piv\n",
    "\tlabels = train_labels\n",
    "\tbias = random.uniform(-1.0,1.0)\n",
    "\t#add bias on the end. Some people add bias on the beginning.\n",
    "\tweights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "\tthreshold = 2.0\n",
    "\tlifecycles = 20\n",
    "\tfit(lifecycles)\n",
    "\t#must activate weighted test values to turn the back to lables 0 or 1\n",
    "\tpiv = test_piv\n",
    "\tlabels = test_labels\n",
    "\ttest_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "contemporary-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percept(piv,lbls):\n",
    "\t# Define the input\n",
    "\t#precise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]]\n",
    "\tprecise_input_labels = piv\n",
    "\t#labels = np.array([0, 0, 0, 1])\n",
    "\tlabels = lbls\n",
    "\tprint(\"piv: \", piv)\n",
    "\tprint(\"lbls: \", lbls)\n",
    "\n",
    "\t#make some random precise data\n",
    "\tprecise_labels = []\n",
    "\tprecise_rounds = []\n",
    "\tfor i in range(20000):\n",
    "\t\tlarge_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "\t\tdonut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "\t\tsmall_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "\t\treg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "\t\tprecise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "\t\tprecise_labels += [1,1,0,0]\n",
    "\n",
    "\t#split the data                               \n",
    "\tratio = int(len(precise_rounds)*.7)           \n",
    "\ttrain_piv = np.array(precise_rounds[:ratio])   \n",
    "\ttrain_labels = np.array(precise_labels[:ratio])\n",
    "\n",
    "\ttest_piv = np.array(precise_rounds[ratio:])   \n",
    "\ttest_labels = np.array(precise_labels[ratio:])\n",
    "\n",
    "\tpiv = train_piv\n",
    "\tlabels = train_labels\n",
    "\tbias = random.uniform(-1.0,1.0)\n",
    "\t#add bias on the end. Some people add bias on the beginning.\n",
    "\trand_weights = np.random.rand(len(precise_rounds[0]) + 1)\n",
    "\tweights = (rand_weights*2)-1 #convert to a float between -1 and 1\n",
    "\tthreshold = 2.0\n",
    "\tlifecycles = 20\n",
    "\tfit(lifecycles)\n",
    "\t#must activate weighted test values to turn the back to lables 0 or 1\n",
    "\tpiv = test_piv\n",
    "\tlabels = test_labels\n",
    "\ttest_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lesbian-supervisor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piv:  [[1.   4.  ]\n",
      " [7.   0.1 ]\n",
      " [1.5  0.28]\n",
      " [1.1  0.8 ]]\n",
      "lbls:  [0 1 0 1]\n",
      "0 2\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 2\n",
      "6 2\n",
      "7 2\n",
      "8 2\n",
      "9 2\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 2\n",
      "15 2\n",
      "16 2\n",
      "17 2\n",
      "18 2\n",
      "19 2\n",
      "no answer found\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "piv = np.array([[1,4], [7,.1], [1.5,.28], [1.1, .8]])\n",
    "lbls = np.array([0, 1, 0, 1])\n",
    "perceptron(piv,lbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "postal-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron():\n",
    "\t# Define the input\n",
    "\tprecise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "\tlabels = np.array([0, 0, 0, 1])\n",
    "\n",
    "\t#make some random precise data\n",
    "\tprecise_labels = []\n",
    "\tprecise_rounds = []\n",
    "\tfor i in range(20000):\n",
    "\t\tlarge_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "\t\tdonut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "\t\tsmall_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "\t\treg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "\t\tprecise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "\t\tprecise_labels += [1,1,0,0]\n",
    "\n",
    "\t#split the data                               \n",
    "\tratio = int(len(precise_rounds)*.7)           \n",
    "\ttrain_piv = np.array(precise_rounds[:ratio])   \n",
    "\ttrain_labels = np.array(precise_labels[:ratio])\n",
    "\n",
    "\ttest_piv = np.array(precise_rounds[ratio:])   \n",
    "\ttest_labels = np.array(precise_labels[ratio:])\n",
    "\n",
    "\tpiv = train_piv\n",
    "\tlabels = train_labels\n",
    "\tbias = random.uniform(-1.0,1.0)\n",
    "\t#add bias on the end. Some people add bias on the beginning.\n",
    "\tweights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "\tthreshold = 2.0\n",
    "\tlifecycles = 20\n",
    "\tfit(lifecycles)\n",
    "\t#must activate weighted test values to turn the back to lables 0 or 1\n",
    "\tpiv = test_piv\n",
    "\tlabels = test_labels\n",
    "\ttest_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "twelve-singles",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "perceptron() missing 2 required positional arguments: 'piv' and 'lbls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-ec725393880e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mperceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: perceptron() missing 2 required positional arguments: 'piv' and 'lbls'"
     ]
    }
   ],
   "source": [
    "perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-colleague",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
