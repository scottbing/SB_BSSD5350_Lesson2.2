{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affecting-basics",
   "metadata": {},
   "source": [
    "# SSB_5350_Perceptron2.2 \n",
    "### Scott Bing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "creative-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "large-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-prior",
   "metadata": {},
   "source": [
    "# Define the required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorporate-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weights(w, act):\n",
    "    err = 0\n",
    "    for i in range(len(piv)):\n",
    "       vs = weighted_sum(piv[i], w[1:]) + weights[0]\n",
    "       if act(vs, threshold) != labels[i]:\n",
    "           err+=1\n",
    "    print(err/len(piv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attempted-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(cycles):\n",
    "    step =  2.0\n",
    "    for c in range(cycles):\n",
    "        #initialize error to 0 for each cycle\n",
    "        num_errors = 0\n",
    "        \n",
    "        for  i in range(len(piv)):\n",
    "            w_sum = weighted_sum(piv[i], weights[1:])\n",
    "            #add bias whdi is first weight\n",
    "            w_sum = weights[0] + w_sum\n",
    "            act = activation(w_sum, threshold)\n",
    "            #determine if the weights yeild right answers\n",
    "            # if so then erro 0 or no error\n",
    "            error = labels[i] - act\n",
    "            \n",
    "            if error != 0:\n",
    "                #increase error count\n",
    "                num_errors += 1\n",
    "#update weights to minimize error in  next cycle\n",
    "                #update weights to minimize error in next cycle\n",
    "                weights[0] += step * error * piv[i][0]\n",
    "                weights[1] += step * error * piv[i][1]\n",
    "#update bias\n",
    "                weights[0] += error\n",
    "        #end fit early if a cycle had no errors\n",
    "        print(c, num_errors)\n",
    "                \n",
    "        if num_errors == 0:\n",
    "            print(c, weights, bias)\n",
    "            break\n",
    "    else:\n",
    "        print(\"no answer found\")\n",
    "    return num_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "technical-engine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(vec, weights):\n",
    "    w_vec = vec*weights;\n",
    "    return w_vec.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "traditional-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(val, threshold):\n",
    "    if val >= threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-municipality",
   "metadata": {},
   "source": [
    "# The Perceptron Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dedicated-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input\n",
    "precise_input_vals = np.array([[.4,.4], [.47,.8], [1.2,.49], [1.1, .8]])\n",
    "labels = np.array([0, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominant-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make some random precise data\n",
    "precise_labels = []\n",
    "precise_rounds = []\n",
    "for i in range(20000):\n",
    "    large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "    donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "    small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "    reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "    precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "    precise_labels += [1,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "artistic-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data                               \n",
    "ratio = int(len(precise_rounds)*.7)           \n",
    "train_piv = np.array(precise_rounds[:ratio])   \n",
    "train_labels = np.array(precise_labels[:ratio])\n",
    "                                            \n",
    "test_piv = np.array(precise_rounds[ratio:])   \n",
    "test_labels = np.array(precise_labels[ratio:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "leading-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 413\n",
      "1 167\n",
      "2 113\n",
      "3 124\n",
      "4 130\n",
      "5 93\n",
      "6 74\n",
      "7 77\n",
      "8 80\n",
      "9 64\n",
      "10 55\n",
      "11 39\n",
      "12 39\n",
      "13 58\n",
      "14 51\n",
      "15 62\n",
      "16 61\n",
      "17 51\n",
      "18 52\n",
      "19 53\n",
      "no answer found\n",
      "0.0002916666666666667\n"
     ]
    }
   ],
   "source": [
    "piv = train_piv\n",
    "labels = train_labels\n",
    "bias = random.uniform(-1.0,1.0)\n",
    "#add bias on the end. Some people add bias on the beginning.\n",
    "weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "threshold = 2.0\n",
    "lifecycles = 20\n",
    "errors = fit(lifecycles)\n",
    "#must activate weighted test values to turn the back to lables 0 or 1\n",
    "piv = test_piv\n",
    "labels = test_labels\n",
    "test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "confidential-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(p,l,s):\n",
    "    # Define the input\n",
    "    precise_input_vals = p\n",
    "    labels = l\n",
    "    #make some random precise data\n",
    "    precise_labels = []\n",
    "    precise_rounds = []\n",
    "    for i in range(s):\n",
    "        large_donut = [random.uniform(2.5, 3.0), random.uniform(2.0, 3.0)]\n",
    "        donut       = [random.uniform(1.5, 2.0), random.uniform(1.0, 2.0)]\n",
    "        small_bagel = [random.uniform(0.0, 0.7), random.uniform(0.0, 0.5)]\n",
    "        reg_bagel   = [random.uniform(0.7, 1.5), random.uniform(0.5, 1.0)]\n",
    "        precise_rounds += [large_donut, donut, small_bagel, reg_bagel]\n",
    "        precise_labels += [1,1,0,0]\n",
    "\n",
    "    #split the data                               \n",
    "    ratio = int(len(precise_rounds)*.7)           \n",
    "    train_piv = np.array(precise_rounds[:ratio])   \n",
    "    train_labels = np.array(precise_labels[:ratio])\n",
    "\n",
    "    test_piv = np.array(precise_rounds[ratio:])   \n",
    "    test_labels = np.array(precise_labels[ratio:])\n",
    "\n",
    "    piv = train_piv\n",
    "    labels = train_labels\n",
    "    bias = random.uniform(-1.0,1.0)\n",
    "    #add bias on the end. Some people add bias on the beginning.\n",
    "    weights = np.array([bias, random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "    threshold = 2.0\n",
    "    lifecycles = 20\n",
    "    errors = fit(lifecycles)\n",
    "    #must activate weighted test values to turn the back to lables 0 or 1\n",
    "    piv = test_piv\n",
    "    labels = test_labels\n",
    "    test_weights(weights, activation) #passing activation funtion as param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mental-hughes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pivs =  [ 0.04266433 -0.67418224]\n",
      "0 5\n",
      "1 0\n",
      "1 [-1.22035637e+03  8.15082929e+02 -1.45420037e-01] -0.020792035779567453\n",
      "0.5\n",
      "pivs =  [-0.83957519 -0.77234594]\n",
      "0 0\n",
      "0 [-1.22035637e+03  8.15082929e+02 -1.45420037e-01] -0.020792035779567453\n",
      "0.5\n",
      "pivs =  [ 0.48270473 -0.86248712]\n",
      "0 0\n",
      "0 [-1.22035637e+03  8.15082929e+02 -1.45420037e-01] -0.020792035779567453\n",
      "0.5\n",
      "pivs =  [-0.80436329  0.96324741]\n",
      "0 0\n",
      "0 [-1.22035637e+03  8.15082929e+02 -1.45420037e-01] -0.020792035779567453\n",
      "0.5\n",
      "pivs =  [0.39009212 0.45769432]\n",
      "0 0\n",
      "0 [-1.22035637e+03  8.15082929e+02 -1.45420037e-01] -0.020792035779567453\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "iter = 5\n",
    "for _ in range(iter):\n",
    "    pivs = np.array([random.uniform(-1.0, 1.0), random.uniform(-1.0, 1.0)])\n",
    "    print(\"pivs = \", pivs)                \n",
    "    lbls = np.array([0, 0, 0, 1])\n",
    "    size = 20000\n",
    "    perceptron(pivs,lbls,size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-protection",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
